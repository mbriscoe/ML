{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c32341f",
   "metadata": {},
   "source": [
    "# Machine Learning Tutorial: A Comprehensive Guide\n",
    "\n",
    "Welcome to this comprehensive machine learning tutorial! In this notebook, we'll explore the fundamentals of machine learning using Python and popular libraries like scikit-learn, pandas, and matplotlib.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Data preprocessing and cleaning techniques\n",
    "- Exploratory data analysis and visualization\n",
    "- Supervised learning algorithms (Linear Regression, Decision Trees, Random Forest)\n",
    "- Model evaluation and comparison\n",
    "- Cross-validation and hyperparameter tuning\n",
    "- Feature importance analysis\n",
    "- Making predictions on new data\n",
    "\n",
    "Let's dive into the exciting world of machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d50cf55",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we'll import all the essential libraries we'll need for this tutorial. Each library serves a specific purpose in our machine learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Python packages versions:\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"seaborn: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db4132",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "We'll use the Boston Housing dataset, which is perfect for regression tasks. This dataset contains information about housing values in Boston suburbs and various features that might influence housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f00e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['PRICE'] = boston.target  # Add the target variable\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of features: {len(boston.feature_names)}\")\n",
    "print(f\"Target variable: House prices in $1000s\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a87d6",
   "metadata": {},
   "source": [
    "### Feature Descriptions:\n",
    "- **CRIM**: Crime rate per capita\n",
    "- **ZN**: Proportion of residential land zoned for lots over 25,000 sq.ft\n",
    "- **INDUS**: Proportion of non-retail business acres\n",
    "- **CHAS**: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- **NOX**: Nitric oxides concentration (parts per 10 million)\n",
    "- **RM**: Average number of rooms per dwelling\n",
    "- **AGE**: Proportion of owner-occupied units built prior to 1940\n",
    "- **DIS**: Weighted distances to employment centers\n",
    "- **RAD**: Index of accessibility to radial highways\n",
    "- **TAX**: Property tax rate per $10,000\n",
    "- **PTRATIO**: Pupil-teacher ratio by town\n",
    "- **B**: Proportion of blacks by town\n",
    "- **LSTAT**: % lower status of the population\n",
    "- **PRICE**: Median home value in $1000s (our target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fef339",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning\n",
    "\n",
    "Before building machine learning models, we need to ensure our data is clean and properly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Look for outliers using the IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers)\n",
    "\n",
    "print(f\"\\nOutlier detection (using IQR method):\")\n",
    "for column in df.columns:\n",
    "    outlier_count = detect_outliers(df, column)\n",
    "    print(f\"{column}: {outlier_count} outliers\")\n",
    "\n",
    "print(f\"\\nDataset is clean! No missing values or duplicates found.\")\n",
    "print(f\"Some outliers detected, but this is normal for real-world data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49730cac",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "Let's visualize our data to understand patterns, distributions, and relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394901e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting area\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Distribution of the target variable (house prices)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(df['PRICE'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of House Prices')\n",
    "plt.xlabel('Price ($1000s)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(2, 3, 2)\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "\n",
    "# Scatter plot: RM (rooms) vs Price\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(df['RM'], df['PRICE'], alpha=0.6)\n",
    "plt.title('Rooms vs House Price')\n",
    "plt.xlabel('Average Number of Rooms')\n",
    "plt.ylabel('Price ($1000s)')\n",
    "\n",
    "# Scatter plot: LSTAT vs Price\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(df['LSTAT'], df['PRICE'], alpha=0.6, color='orange')\n",
    "plt.title('Lower Status % vs House Price')\n",
    "plt.xlabel('% Lower Status Population')\n",
    "plt.ylabel('Price ($1000s)')\n",
    "\n",
    "# Box plot: CHAS (Charles River) vs Price\n",
    "plt.subplot(2, 3, 5)\n",
    "df.boxplot(column='PRICE', by='CHAS', ax=plt.gca())\n",
    "plt.title('House Prices by Charles River Location')\n",
    "plt.xlabel('Charles River (0=No, 1=Yes)')\n",
    "plt.ylabel('Price ($1000s)')\n",
    "\n",
    "# Crime rate vs Price\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(df['CRIM'], df['PRICE'], alpha=0.6, color='red')\n",
    "plt.title('Crime Rate vs House Price')\n",
    "plt.xlabel('Crime Rate per Capita')\n",
    "plt.ylabel('Price ($1000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top correlations with price\n",
    "price_corr = df.corr()['PRICE'].abs().sort_values(ascending=False)\n",
    "print(\"Features most correlated with house price:\")\n",
    "print(price_corr[1:6])  # Exclude price itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717a302",
   "metadata": {},
   "source": [
    "## 5. Feature Selection and Engineering\n",
    "\n",
    "We'll prepare our features for machine learning by selecting the most relevant ones and applying necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e162b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop('PRICE', axis=1)  # Features\n",
    "y = df['PRICE']               # Target variable\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "\n",
    "# Let's create some new features that might be useful\n",
    "# Feature engineering: Create new meaningful features\n",
    "\n",
    "# 1. Rooms per capita (rooms divided by population density)\n",
    "X['ROOMS_PER_CAPITA'] = X['RM'] / (X['CRIM'] + 1)  # Adding 1 to avoid division by zero\n",
    "\n",
    "# 2. Tax per room (property tax divided by number of rooms)\n",
    "X['TAX_PER_ROOM'] = X['TAX'] / X['RM']\n",
    "\n",
    "# 3. Distance-accessibility interaction\n",
    "X['DIS_RAD_INTERACTION'] = X['DIS'] * X['RAD']\n",
    "\n",
    "print(\"\\nNew features created:\")\n",
    "print(\"- ROOMS_PER_CAPITA: Rooms relative to crime rate\")\n",
    "print(\"- TAX_PER_ROOM: Property tax per room\")\n",
    "print(\"- DIS_RAD_INTERACTION: Distance-accessibility interaction\")\n",
    "\n",
    "# Select features based on correlation with target\n",
    "feature_importance = abs(X.corrwith(y)).sort_values(ascending=False)\n",
    "print(\"\\nFeature importance (correlation with price):\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Select top features for modeling\n",
    "top_features = feature_importance.head(8).index.tolist()\n",
    "X_selected = X[top_features]\n",
    "\n",
    "print(f\"\\nSelected {len(top_features)} most important features:\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275d921",
   "metadata": {},
   "source": [
    "## 6. Split Data into Training and Testing Sets\n",
    "\n",
    "We'll split our data to train our models on one portion and test them on unseen data to evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(\"Data split completed!\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Scale the features for algorithms that are sensitive to feature scales\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling completed!\")\n",
    "print(f\"Original feature ranges:\")\n",
    "print(f\"Min values: {X_train.min()}\")\n",
    "print(f\"Max values: {X_train.max()}\")\n",
    "print(f\"\\nScaled feature ranges:\")\n",
    "print(f\"Min values: {X_train_scaled.min(axis=0)}\")\n",
    "print(f\"Max values: {X_train_scaled.max(axis=0)}\")\n",
    "\n",
    "# Convert scaled arrays back to DataFrames for easier handling\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2c877b",
   "metadata": {},
   "source": [
    "## 7. Train a Linear Regression Model\n",
    "\n",
    "Linear regression is one of the simplest and most interpretable machine learning algorithms. It assumes a linear relationship between features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dd7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate performance metrics\n",
    "lr_mse = mean_squared_error(y_test, lr_predictions)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_r2 = r2_score(y_test, lr_predictions)\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {lr_mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {lr_rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {lr_r2:.3f}\")\n",
    "\n",
    "# Display feature coefficients\n",
    "feature_coefficients = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lr_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Coefficients (sorted by absolute value):\")\n",
    "print(feature_coefficients)\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, lr_predictions, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - lr_predictions\n",
    "plt.scatter(lr_predictions, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d88df",
   "metadata": {},
   "source": [
    "## 8. Train a Decision Tree Model\n",
    "\n",
    "Decision trees are intuitive models that make decisions by asking a series of questions about the features. They're highly interpretable and can capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac39399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Decision Tree model\n",
    "# Note: Decision trees don't require feature scaling, so we'll use the original features\n",
    "dt_model = DecisionTreeRegressor(max_depth=6, min_samples_split=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
    "dt_rmse = np.sqrt(dt_mse)\n",
    "dt_r2 = r2_score(y_test, dt_predictions)\n",
    "\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {dt_mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {dt_rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {dt_r2:.3f}\")\n",
    "\n",
    "# Feature importance from the decision tree\n",
    "dt_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Decision Tree):\")\n",
    "print(dt_feature_importance)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Predictions vs Actual\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test, dt_predictions, alpha=0.6, color='green')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Decision Tree: Actual vs Predicted')\n",
    "\n",
    "# Feature importance plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(dt_feature_importance['Feature'], dt_feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Decision Tree)')\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 3, 3)\n",
    "dt_residuals = y_test - dt_predictions\n",
    "plt.scatter(dt_predictions, dt_residuals, alpha=0.6, color='green')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Decision Tree)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some information about the tree\n",
    "print(f\"\\nDecision Tree Information:\")\n",
    "print(f\"Tree depth: {dt_model.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dae218",
   "metadata": {},
   "source": [
    "## 9. Train a Random Forest Model\n",
    "\n",
    "Random Forest is an ensemble method that combines multiple decision trees to create a more robust and accurate model. It reduces overfitting and provides better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,        # Number of trees in the forest\n",
    "    max_depth=8,             # Maximum depth of trees\n",
    "    min_samples_split=5,     # Minimum samples required to split a node\n",
    "    min_samples_leaf=2,      # Minimum samples required at a leaf node\n",
    "    random_state=42          # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_r2 = r2_score(y_test, rf_predictions)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {rf_mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rf_rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {rf_r2:.3f}\")\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Random Forest):\")\n",
    "print(rf_feature_importance)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Predictions vs Actual\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test, rf_predictions, alpha=0.6, color='purple')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Random Forest: Actual vs Predicted')\n",
    "\n",
    "# Feature importance comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.barh(rf_feature_importance['Feature'], rf_feature_importance['Importance'], color='purple', alpha=0.7)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 3, 3)\n",
    "rf_residuals = y_test - rf_predictions\n",
    "plt.scatter(rf_predictions, rf_residuals, alpha=0.6, color='purple')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Random Forest)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRandom Forest Information:\")\n",
    "print(f\"Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"Out-of-bag score: {rf_model.oob_score_:.3f}\" if hasattr(rf_model, 'oob_score_') else \"OOB score not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f8727",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Comparison\n",
    "\n",
    "Let's compare all three models to see which one performs best on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c8bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'MSE': [lr_mse, dt_mse, rf_mse],\n",
    "    'RMSE': [lr_rmse, dt_rmse, rf_rmse],\n",
    "    'RÂ² Score': [lr_r2, dt_r2, rf_r2]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# MSE Comparison\n",
    "axes[0, 0].bar(model_comparison['Model'], model_comparison['MSE'], \n",
    "               color=['blue', 'green', 'purple'], alpha=0.7)\n",
    "axes[0, 0].set_title('Mean Squared Error Comparison')\n",
    "axes[0, 0].set_ylabel('MSE')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RÂ² Score Comparison\n",
    "axes[0, 1].bar(model_comparison['Model'], model_comparison['RÂ² Score'], \n",
    "               color=['blue', 'green', 'purple'], alpha=0.7)\n",
    "axes[0, 1].set_title('RÂ² Score Comparison')\n",
    "axes[0, 1].set_ylabel('RÂ² Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Prediction comparison plot\n",
    "axes[1, 0].scatter(y_test, lr_predictions, alpha=0.5, label='Linear Regression', color='blue')\n",
    "axes[1, 0].scatter(y_test, dt_predictions, alpha=0.5, label='Decision Tree', color='green')\n",
    "axes[1, 0].scatter(y_test, rf_predictions, alpha=0.5, label='Random Forest', color='purple')\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('Actual Prices')\n",
    "axes[1, 0].set_ylabel('Predicted Prices')\n",
    "axes[1, 0].set_title('All Models: Actual vs Predicted')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Residuals comparison\n",
    "axes[1, 1].scatter(lr_predictions, lr_residuals, alpha=0.5, label='Linear Regression', color='blue')\n",
    "axes[1, 1].scatter(dt_predictions, dt_residuals, alpha=0.5, label='Decision Tree', color='green')\n",
    "axes[1, 1].scatter(rf_predictions, rf_residuals, alpha=0.5, label='Random Forest', color='purple')\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Predicted Prices')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title('Residuals Comparison')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the best model\n",
    "best_model_idx = model_comparison['RÂ² Score'].idxmax()\n",
    "best_model = model_comparison.loc[best_model_idx, 'Model']\n",
    "best_r2 = model_comparison.loc[best_model_idx, 'RÂ² Score']\n",
    "\n",
    "print(f\"\\nðŸ† Best performing model: {best_model}\")\n",
    "print(f\"   RÂ² Score: {best_r2:.3f}\")\n",
    "print(f\"   This means the model explains {best_r2*100:.1f}% of the variance in house prices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d2cab",
   "metadata": {},
   "source": [
    "## 11. Cross-Validation\n",
    "\n",
    "Cross-validation helps us get a more robust estimate of model performance by training and testing on different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78804c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation for all models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Prepare models for cross-validation\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=6, min_samples_split=10, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    if name == 'Linear Regression':\n",
    "        # Use scaled features for Linear Regression\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    else:\n",
    "        # Use original features for tree-based models\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'scores': cv_scores,\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"5-Fold Cross-Validation Results (RÂ² Score):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cv_summary = []\n",
    "for name, results in cv_results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean RÂ² Score: {results['mean']:.3f} (Â±{results['std']:.3f})\")\n",
    "    print(f\"  Individual folds: {[f'{score:.3f}' for score in results['scores']]}\")\n",
    "    print()\n",
    "    \n",
    "    cv_summary.append({\n",
    "        'Model': name,\n",
    "        'Mean RÂ²': results['mean'],\n",
    "        'Std RÂ²': results['std']\n",
    "    })\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "cv_df = pd.DataFrame(cv_summary)\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Bar plot of mean scores with error bars\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(cv_df['Model'], cv_df['Mean RÂ²'], \n",
    "        yerr=cv_df['Std RÂ²'], capsize=5, \n",
    "        color=['blue', 'green', 'purple'], alpha=0.7)\n",
    "plt.title('Cross-Validation Results (Mean Â± Std)')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Box plot of all CV scores\n",
    "plt.subplot(1, 2, 2)\n",
    "cv_scores_list = [cv_results[name]['scores'] for name in models.keys()]\n",
    "plt.boxplot(cv_scores_list, labels=models.keys())\n",
    "plt.title('Distribution of CV Scores')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the best model based on cross-validation\n",
    "best_cv_model = cv_df.loc[cv_df['Mean RÂ²'].idxmax(), 'Model']\n",
    "best_cv_score = cv_df.loc[cv_df['Mean RÂ²'].idxmax(), 'Mean RÂ²']\n",
    "\n",
    "print(f\"ðŸ† Best model based on cross-validation: {best_cv_model}\")\n",
    "print(f\"   Mean RÂ² Score: {best_cv_score:.3f}\")\n",
    "print(f\"   This provides a more robust estimate of model performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee07cb6",
   "metadata": {},
   "source": [
    "## 12. Hyperparameter Tuning\n",
    "\n",
    "Let's optimize our best performing model by finding the optimal hyperparameters using Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cdafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tune the Random Forest model (assuming it's our best performer)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],           # Number of trees\n",
    "    'max_depth': [6, 8, 10, None],           # Maximum depth of trees\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum samples to split\n",
    "    'min_samples_leaf': [1, 2, 4],           # Minimum samples at leaf\n",
    "    'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider\n",
    "}\n",
    "\n",
    "print(\"Starting hyperparameter tuning for Random Forest...\")\n",
    "print(f\"Total combinations to test: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf']) * len(param_grid['max_features'])}\")\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='r2',            # RÂ² score as the metric\n",
    "    n_jobs=-1,              # Use all available cores\n",
    "    verbose=1               # Show progress\n",
    ")\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Hyperparameter tuning completed!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Make predictions with the tuned model\n",
    "best_rf_predictions = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics for the tuned model\n",
    "best_rf_mse = mean_squared_error(y_test, best_rf_predictions)\n",
    "best_rf_rmse = np.sqrt(best_rf_mse)\n",
    "best_rf_r2 = r2_score(y_test, best_rf_predictions)\n",
    "\n",
    "print(f\"\\nTuned Random Forest Results:\")\n",
    "print(f\"Mean Squared Error (MSE): {best_rf_mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {best_rf_rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {best_rf_r2:.3f}\")\n",
    "\n",
    "# Compare with original Random Forest\n",
    "print(f\"\\nImprovement over original Random Forest:\")\n",
    "print(f\"MSE improvement: {rf_mse - best_rf_mse:.2f}\")\n",
    "print(f\"RÂ² improvement: {best_rf_r2 - rf_r2:.3f}\")\n",
    "\n",
    "# Visualize the improvement\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Comparison of RÂ² scores\n",
    "models_comparison = ['Original RF', 'Tuned RF']\n",
    "r2_scores = [rf_r2, best_rf_r2]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(models_comparison, r2_scores, color=['purple', 'darkblue'], alpha=0.7)\n",
    "plt.title('Model Performance: Before vs After Tuning')\n",
    "plt.ylabel('RÂ² Score')\n",
    "\n",
    "# Predictions comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_test, rf_predictions, alpha=0.5, label='Original RF', color='purple')\n",
    "plt.scatter(y_test, best_rf_predictions, alpha=0.5, label='Tuned RF', color='darkblue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Predictions: Original vs Tuned RF')\n",
    "plt.legend()\n",
    "\n",
    "# Feature importance for tuned model\n",
    "best_rf_feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.barh(best_rf_feature_importance['Feature'], best_rf_feature_importance['Importance'], \n",
    "         color='darkblue', alpha=0.7)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Tuned RF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 5 most important features (tuned model):\")\n",
    "print(best_rf_feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1745c1da",
   "metadata": {},
   "source": [
    "## 13. Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for predicting house prices and understand how our model makes decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive feature importance analysis\n",
    "print(\"Feature Importance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Feature importance from our best model\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_rf_model.feature_importances_,\n",
    "    'Importance_Percentage': best_rf_model.feature_importances_ * 100\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Rankings:\")\n",
    "for idx, row in feature_importance_df.iterrows():\n",
    "    print(f\"{row.name + 1:2d}. {row['Feature']:20s} - {row['Importance_Percentage']:5.1f}%\")\n",
    "\n",
    "# 2. Correlation with target variable\n",
    "target_correlation = abs(X_train.corrwith(y_train)).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nCorrelation with house prices:\")\n",
    "for feature in target_correlation.index[:5]:\n",
    "    corr = X_train[feature].corr(y_train)\n",
    "    print(f\"{feature:20s} - {corr:6.3f}\")\n",
    "\n",
    "# 3. Visualize feature importance\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Main feature importance plot\n",
    "plt.subplot(2, 2, 1)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(feature_importance_df)))\n",
    "plt.barh(range(len(feature_importance_df)), feature_importance_df['Importance'], color=colors)\n",
    "plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Top 5 features pie chart\n",
    "plt.subplot(2, 2, 2)\n",
    "top_5_features = feature_importance_df.head(5)\n",
    "plt.pie(top_5_features['Importance'], labels=top_5_features['Feature'], autopct='%1.1f%%')\n",
    "plt.title('Top 5 Features Distribution')\n",
    "\n",
    "# Feature importance vs correlation\n",
    "plt.subplot(2, 2, 3)\n",
    "correlation_values = [abs(X_train[feature].corr(y_train)) for feature in feature_importance_df['Feature']]\n",
    "plt.scatter(feature_importance_df['Importance'], correlation_values, alpha=0.7)\n",
    "plt.xlabel('Random Forest Importance')\n",
    "plt.ylabel('Correlation with Target')\n",
    "plt.title('Importance vs Correlation')\n",
    "\n",
    "# Add feature names to points\n",
    "for i, feature in enumerate(feature_importance_df['Feature']):\n",
    "    plt.annotate(feature, (feature_importance_df.iloc[i]['Importance'], correlation_values[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Cumulative importance\n",
    "plt.subplot(2, 2, 4)\n",
    "cumulative_importance = np.cumsum(feature_importance_df['Importance'])\n",
    "plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'bo-')\n",
    "plt.axhline(y=0.8, color='r', linestyle='--', label='80% threshold')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.title('Cumulative Feature Importance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Feature insights\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"â€¢ Most important feature: {feature_importance_df.iloc[0]['Feature']} ({feature_importance_df.iloc[0]['Importance_Percentage']:.1f}%)\")\n",
    "print(f\"â€¢ Top 3 features account for {feature_importance_df.head(3)['Importance_Percentage'].sum():.1f}% of importance\")\n",
    "\n",
    "# Find how many features are needed for 80% importance\n",
    "cumsum = np.cumsum(feature_importance_df['Importance'])\n",
    "features_for_80_percent = np.where(cumsum >= 0.8)[0][0] + 1\n",
    "print(f\"â€¢ {features_for_80_percent} features needed to explain 80% of model decisions\")\n",
    "\n",
    "# 5. Feature descriptions based on importance\n",
    "feature_descriptions = {\n",
    "    'LSTAT': 'Lower status population percentage - strong negative correlation with price',\n",
    "    'RM': 'Average number of rooms - more rooms typically mean higher prices',\n",
    "    'NOX': 'Nitric oxide concentration - pollution indicator affecting desirability',\n",
    "    'DIS': 'Distance to employment centers - accessibility factor',\n",
    "    'CRIM': 'Crime rate - safety factor affecting property values',\n",
    "    'AGE': 'Age of buildings - newer buildings often more valuable',\n",
    "    'TAX': 'Property tax rate - affects affordability and attractiveness',\n",
    "    'PTRATIO': 'Pupil-teacher ratio - education quality indicator',\n",
    "    'CHAS': 'Charles River proximity - waterfront premium',\n",
    "    'INDUS': 'Industrial area proportion - affects desirability',\n",
    "    'ZN': 'Large lot zoning - affects neighborhood character',\n",
    "    'RAD': 'Highway accessibility - transportation convenience',\n",
    "    'B': 'Racial composition statistic'\n",
    "}\n",
    "\n",
    "print(f\"\\nTop Features Explained:\")\n",
    "for i, feature in enumerate(feature_importance_df.head(5)['Feature']):\n",
    "    description = feature_descriptions.get(feature, 'No description available')\n",
    "    print(f\"{i+1}. {feature}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053df89",
   "metadata": {},
   "source": [
    "## 14. Make Predictions on New Data\n",
    "\n",
    "Finally, let's demonstrate how to use our trained model to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some hypothetical new houses for prediction\n",
    "# Let's create realistic scenarios based on our feature understanding\n",
    "\n",
    "new_houses = pd.DataFrame({\n",
    "    'CRIM': [0.1, 5.0, 0.05],           # Low, high, very low crime rate\n",
    "    'ZN': [20.0, 0.0, 50.0],            # Various zoning\n",
    "    'INDUS': [5.0, 15.0, 2.0],          # Industrial proportion\n",
    "    'CHAS': [1, 0, 1],                  # Charles River: Yes, No, Yes\n",
    "    'NOX': [0.4, 0.7, 0.3],             # Low, high, very low pollution\n",
    "    'RM': [7.5, 5.0, 8.2],              # 7.5, 5, 8.2 rooms\n",
    "    'AGE': [30.0, 80.0, 10.0],          # Building age\n",
    "    'DIS': [4.0, 2.0, 6.0],             # Distance to employment\n",
    "    'RAD': [3, 24, 2],                  # Highway access\n",
    "    'TAX': [300, 600, 250],             # Property tax\n",
    "    'PTRATIO': [15.0, 20.0, 12.0],      # Pupil-teacher ratio\n",
    "    'B': [390.0, 300.0, 395.0],         # Racial composition\n",
    "    'LSTAT': [5.0, 25.0, 3.0]           # Lower status %\n",
    "})\n",
    "\n",
    "# Create the engineered features for new houses\n",
    "new_houses['ROOMS_PER_CAPITA'] = new_houses['RM'] / (new_houses['CRIM'] + 1)\n",
    "new_houses['TAX_PER_ROOM'] = new_houses['TAX'] / new_houses['RM']\n",
    "new_houses['DIS_RAD_INTERACTION'] = new_houses['DIS'] * new_houses['RAD']\n",
    "\n",
    "# Select the same features used in training\n",
    "new_houses_selected = new_houses[top_features]\n",
    "\n",
    "print(\"New Houses Data:\")\n",
    "print(\"=\" * 60)\n",
    "print(new_houses_selected)\n",
    "\n",
    "# Make predictions using our best model\n",
    "predictions = best_rf_model.predict(new_houses_selected)\n",
    "\n",
    "# Create a results summary\n",
    "house_descriptions = [\n",
    "    \"Luxury House: Low crime, waterfront, many rooms, low pollution\",\n",
    "    \"Budget House: High crime, no waterfront, fewer rooms, high pollution\", \n",
    "    \"Premium House: Very low crime, waterfront, many rooms, excellent area\"\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'House Description': house_descriptions,\n",
    "    'Predicted Price ($1000s)': predictions,\n",
    "    'Predicted Price ($)': predictions * 1000\n",
    "})\n",
    "\n",
    "print(f\"\\nPrediction Results:\")\n",
    "print(\"=\" * 60)\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"\\n{idx + 1}. {row['House Description']}\")\n",
    "    print(f\"   Predicted Price: ${row['Predicted Price ($)']:,.0f}\")\n",
    "    print(f\"   (${row['Predicted Price ($1000s)']:.1f}k)\")\n",
    "\n",
    "# Let's also create a prediction confidence analysis\n",
    "# Make multiple predictions with different random states to estimate uncertainty\n",
    "n_predictions = 100\n",
    "uncertainty_predictions = []\n",
    "\n",
    "for i in range(n_predictions):\n",
    "    # Create a new model with different random state\n",
    "    temp_model = RandomForestRegressor(**best_rf_model.get_params())\n",
    "    temp_model.set_params(random_state=i)\n",
    "    temp_model.fit(X_train, y_train)\n",
    "    temp_predictions = temp_model.predict(new_houses_selected)\n",
    "    uncertainty_predictions.append(temp_predictions)\n",
    "\n",
    "uncertainty_predictions = np.array(uncertainty_predictions)\n",
    "\n",
    "# Calculate prediction intervals\n",
    "prediction_mean = uncertainty_predictions.mean(axis=0)\n",
    "prediction_std = uncertainty_predictions.std(axis=0)\n",
    "lower_bound = prediction_mean - 1.96 * prediction_std\n",
    "upper_bound = prediction_mean + 1.96 * prediction_std\n",
    "\n",
    "print(f\"\\nPrediction Confidence Intervals (95%):\")\n",
    "print(\"=\" * 60)\n",
    "for i, description in enumerate(house_descriptions):\n",
    "    print(f\"\\n{i + 1}. {description}\")\n",
    "    print(f\"   Mean Prediction: ${prediction_mean[i] * 1000:,.0f}\")\n",
    "    print(f\"   95% Confidence Interval: ${lower_bound[i] * 1000:,.0f} - ${upper_bound[i] * 1000:,.0f}\")\n",
    "    print(f\"   Uncertainty: Â±${prediction_std[i] * 1000:,.0f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Bar plot of predictions\n",
    "plt.subplot(2, 2, 1)\n",
    "colors = ['green', 'orange', 'blue']\n",
    "bars = plt.bar(range(len(predictions)), predictions, color=colors, alpha=0.7)\n",
    "plt.xlabel('House Number')\n",
    "plt.ylabel('Predicted Price ($1000s)')\n",
    "plt.title('House Price Predictions')\n",
    "plt.xticks(range(len(predictions)), ['House 1\\n(Luxury)', 'House 2\\n(Budget)', 'House 3\\n(Premium)'])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, pred in zip(bars, predictions):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'${pred:.1f}k', ha='center', va='bottom')\n",
    "\n",
    "# Feature comparison for the three houses\n",
    "plt.subplot(2, 2, 2)\n",
    "important_features = ['LSTAT', 'RM', 'NOX', 'DIS']\n",
    "x = np.arange(len(important_features))\n",
    "width = 0.25\n",
    "\n",
    "for i in range(3):\n",
    "    values = [new_houses_selected.iloc[i][feature] for feature in important_features]\n",
    "    plt.bar(x + i*width, values, width, label=f'House {i+1}', color=colors[i], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Feature Values')\n",
    "plt.title('Key Features Comparison')\n",
    "plt.xticks(x + width, important_features)\n",
    "plt.legend()\n",
    "\n",
    "# Prediction uncertainty\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.errorbar(range(len(prediction_mean)), prediction_mean, yerr=1.96*prediction_std, \n",
    "             fmt='o', capsize=5, capthick=2, color='red')\n",
    "plt.xlabel('House Number')\n",
    "plt.ylabel('Predicted Price ($1000s)')\n",
    "plt.title('Predictions with Confidence Intervals')\n",
    "plt.xticks(range(len(predictions)), ['House 1', 'House 2', 'House 3'])\n",
    "\n",
    "# Feature importance for new predictions\n",
    "plt.subplot(2, 2, 4)\n",
    "# Calculate how much each feature contributes to the difference from average\n",
    "avg_price = y_train.mean()\n",
    "feature_contributions = []\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    contributions = []\n",
    "    for feature in important_features:\n",
    "        feature_idx = list(new_houses_selected.columns).index(feature)\n",
    "        # Simplified contribution calculation (this is approximate)\n",
    "        feature_importance = best_rf_model.feature_importances_[feature_idx]\n",
    "        feature_value = new_houses_selected.iloc[i][feature]\n",
    "        feature_mean = X_train[feature].mean()\n",
    "        contribution = feature_importance * (feature_value - feature_mean) / feature_mean\n",
    "        contributions.append(contribution)\n",
    "    feature_contributions.append(contributions)\n",
    "\n",
    "# Plot contributions for House 1 (example)\n",
    "plt.barh(important_features, feature_contributions[0], color='green', alpha=0.7)\n",
    "plt.xlabel('Relative Feature Contribution')\n",
    "plt.title('Feature Contributions (House 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Summary:\")\n",
    "print(f\"Our machine learning model successfully predicts house prices!\")\n",
    "print(f\"Key factors: Low crime, more rooms, less pollution, better location\")\n",
    "print(f\"Price range: ${min(predictions)*1000:,.0f} - ${max(predictions)*1000:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea1e8a",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations! You've Completed the Machine Learning Tutorial\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Data Loading and Exploration**: How to load datasets and understand their structure\n",
    "2. **Data Preprocessing**: Cleaning data, handling missing values, and feature engineering\n",
    "3. **Exploratory Data Analysis**: Visualizing data patterns and relationships\n",
    "4. **Feature Selection**: Identifying the most important features for modeling\n",
    "5. **Model Training**: Implementing Linear Regression, Decision Trees, and Random Forest\n",
    "6. **Model Evaluation**: Using metrics like MSE, RMSE, and RÂ² to assess performance\n",
    "7. **Cross-Validation**: Getting robust performance estimates\n",
    "8. **Hyperparameter Tuning**: Optimizing model parameters for better performance\n",
    "9. **Feature Importance**: Understanding which features drive predictions\n",
    "10. **Making Predictions**: Using trained models on new, unseen data\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Random Forest** often performs better than single models due to ensemble learning\n",
    "- **Feature engineering** can significantly improve model performance\n",
    "- **Cross-validation** provides more reliable performance estimates than single train-test splits\n",
    "- **Hyperparameter tuning** can squeeze out extra performance from your models\n",
    "- **Feature importance** helps understand and trust your model's decisions\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different datasets and problem types (classification vs regression)\n",
    "- Explore more advanced algorithms (XGBoost, Neural Networks, SVM)\n",
    "- Learn about feature selection techniques (RFE, SelectKBest)\n",
    "- Study model interpretation techniques (SHAP, LIME)\n",
    "- Practice with real-world datasets from Kaggle or UCI ML Repository\n",
    "\n",
    "### Resources for Further Learning:\n",
    "\n",
    "- **Scikit-learn Documentation**: Comprehensive guide to ML in Python\n",
    "- **Kaggle Learn**: Free micro-courses on machine learning topics\n",
    "- **Coursera/edX**: University-level ML courses\n",
    "- **Hands-On Machine Learning** by AurÃ©lien GÃ©ron: Excellent ML book\n",
    "- **Python Machine Learning** by Sebastian Raschka: Deep dive into ML concepts\n",
    "\n",
    "Happy machine learning! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
